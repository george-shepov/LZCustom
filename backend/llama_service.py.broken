import asyncio
import aiohttp
import time
import os
from enum import Enum
from typing import Dict, Optional
from dataclasses import dataclass

class ModelTier(Enum):
    FAST = "fast"
    MEDIUM = "medium"
    ADVANCED = "advanced"
    EXPERT = "expert"

@dataclass
class ModelConfig:
    model_name: str
    timeout: int
    max_tokens: int

class QuestionClassifier:
    @staticmethod
    def classify_question(question: str) -> ModelTier:
        # Simple classification logic
        if len(question) < 20:
            return ModelTier.FAST
        elif len(question) < 100:
            return ModelTier.MEDIUM
        else:
            return ModelTier.ADVANCED

class LLaMAService:
    def __init__(self, base_url: str = None):
        self.base_url = base_url or os.environ.get('OLLAMA_HOST', 'http://localhost:11434')
        self.session = None
        self.classifier = QuestionClassifier()
        
        # Model configurations
        self.models = {
            ModelTier.FAST: ModelConfig("qwen2.5:7b-instruct-q4_k_m", 10, 150),
            ModelTier.MEDIUM: ModelConfig("qwen2.5:7b-instruct-q4_k_m", 20, 300),
            ModelTier.ADVANCED: ModelConfig("qwen2.5:7b-instruct-q4_k_m", 30, 500),
            ModelTier.EXPERT: ModelConfig("qwen2.5:7b-instruct-q4_k_m", 45, 800)
        }

    async def __aenter__(self):
        # Use proper timeout configuration
        timeout = aiohttp.ClientTimeout(total=60)
        self.session = aiohttp.ClientSession(timeout=timeout)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def chat(self, question: str, session_id: str) -> Dict:
        """Main chat interface"""
        return await self.generate_response(question)

    async def generate_response(self, question: str, tier: ModelTier = None) -> Dict:
        """Generate response with proper exception handling"""
        start_time = time.time()
        
        try:
            # Get model config
            if tier is None:
                tier = self.classifier.classify_question(question)
            
            config = self.models.get(tier, self.models[ModelTier.FAST])
            
            # Call Ollama
            response = await self._call_ollama(question, config)
            response_time = time.time() - start_time
            
            return {
                "response": response,
                "model_used": config.model_name,
                "tier": tier.name,
                "response_time": round(response_time, 2),
                "success": True
            }
            
        except Exception as e:
            # Catch all exceptions properly
            print(f"âŒ LLaMA Error: {e}")
            return self._error_response(str(e))

    async def _call_ollama(self, question: str, config: ModelConfig) -> str:
        """Make API call to Ollama with proper exception handling"""
        if not self.session:
            return "Service not initialized"
            
        try:
            # Create proper timeout
            timeout = aiohttp.ClientTimeout(total=config.timeout)
            
            payload = {
                "model": config.model_name,
                "prompt": f"You are a helpful assistant for LZ Custom Fabrication. {question}",
                "stream": False
            }
            
            async with self.session.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=timeout
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    return data.get('response', 'No response generated')
                else:
                    error_text = await response.text()
                    print(f"âŒ Ollama API error {response.status}: {error_text}")
                    raise Exception(f"Ollama API error: {response.status}")
                    
        except asyncio.TimeoutError:
            print(f"â° Ollama timeout after {config.timeout}s")
            raise Exception(f"Ollama timeout: {config.timeout}s")
        except aiohttp.ClientError as e:
            print(f"ðŸ”Œ Ollama connection error: {e}")
            raise Exception(f"Ollama connection error: {e}")
        except Exception as e:
            print(f"âŒ Failed to call Ollama: {type(e).__name__}: {e}")
            raise Exception(f"Failed to call Ollama: {type(e).__name__}: {e}")

    def _error_response(self, error: str) -> Dict:
        """Error response"""
        return {
            "response": "I'm having trouble processing your question right now. Please call us at 216-268-2990 or fill out our quote form, and we'll get back to you within 2 hours!",
            "model_used": "error",
            "tier": "ERROR",
            "response_time": 0,
            "success": False,
            "error": error
        }

    def _fallback_response(self, question: str) -> Dict:
        """Fallback response when AI is unavailable"""
        return {
            "response": "I'm having trouble processing your question right now. Please call us at 216-268-2990 or fill out our quote form, and we'll get back to you within 2 hours!",
            "model_used": "fallback",
            "tier": "FALLBACK",
            "response_time": 0,
            "success": False
        }
